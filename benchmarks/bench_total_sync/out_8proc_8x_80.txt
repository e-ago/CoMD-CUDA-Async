+ local A=0
+ local B=0
+ local C=1
+ local D=1
+ local NP=8
+ shift 5
+ local 'PAR=-e -i 8 -j 1 -k 1 -x 80 -y 80 -z 80'
+ date
mer 23 nov 2016, 09.33.50, GMT
+ tee -a run.log
+ echo

+ echo

+ mpirun -x ASYNC_USE_ASYNC=0 -x ASYNC_ENABLE_DEBUG=0 -x COMM_USE_COMM=0 -x COMM_USE_ASYNC=0 -x COMM_USE_GPU_COMM=0 -x MP_ENABLE_DEBUG=0 -x GDS_ENABLE_DEBUG=0 -x ENABLE_DEBUG_MSG=0 -x MLX5_DEBUG_MASK=0 -x MLX5_FREEZE_ON_ERROR_CQE=0 -x MP_DBREC_ON_GPU=0 -x MP_RX_CQ_ON_GPU=0 -x MP_TX_CQ_ON_GPU=0 -x MP_EVENT_ASYNC=0 -x MP_GUARD_PROGRESS=0 -x GDS_DISABLE_WRITE64=0 -x GDS_SIMULATE_WRITE64=0 -x GDS_DISABLE_INLINECOPY=0 -x GDS_ENABLE_WEAK_CONSISTENCY=1 -x GDS_DISABLE_MEMBAR=1 -x CUDA_VISIBLE_DEVICES=0 -x CUDA_DISABLE_UNIFIED_MEMORY=0 --mca btl_openib_want_cuda_gdr 1 --map-by node -np 8 -mca btl_openib_warn_default_gid_prefix 0 /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi -e -i 8 -j 1 -k 1 -x 80 -y 80 -z 80
[tesla27:44899] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla29:00586] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla28:10608] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla30:06498] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla57:38967] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla58:37525] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla59:36556] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla31:04737] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
Wed Nov 23 09:33:52 2016: Starting Initialization


Mini-Application Name    : CoMD-cuda-mpi
Mini-Application Version : 1.1
Platform:
  hostname: tesla110
  kernel name: 'Linux'
  kernel release: '3.10.0-327.36.3.el7.x86_64'
  processor: 'x86_64'
Build:
  CC: '/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc'
  compiler version: 'gcc (GCC) 5.3.0'
  CFLAGS: '-std=c99 -Wno-unused-result -DMAXATOMS=64  -DNDEBUG -g -O5 -DCOMD_DOUBLE -DDO_MPI  -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/hwloc/hwloc191/hwloc/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi -I/usr/local/Cluster-Apps/cuda/8.0/include'
  LDFLAGS: '-L/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib -lmpi -lm -lstdc++ -L/usr/local/Cluster-Apps/cuda/8.0/lib64 -lcudart'
  using MPI: true
  Threading: none
  Double Precision: true
Run Date/Time: 2016-11-23, 09:33:52

Command Line Parameters:
  doeam: 1
  potDir: pots
  potName: Cu_u6.eam
  potType: funcfl
  nx: 80
  ny: 80
  nz: 80
  xproc: 8
  yproc: 1
  zproc: 1
  Lattice constant: -1 Angstroms
  nSteps: 100
  printRate: 10
  Time step: 1 fs
  Initial Temperature: 600 K
  Initial Delta: 0 Angstroms

  GPU async opt: 0
  GPU profiling mode: 0
  GPU method: thread_atom
  Space-filling (Hilbert): 0

Host tesla27 using GPU 0: Tesla K20c

Host tesla29 using GPU 0: Tesla K20c

Host tesla28 using GPU 0: Tesla K20c

Host tesla31 using GPU 0: Tesla K20c

Host tesla58 using GPU 0: Tesla K20c

Host tesla57 using GPU 0: Tesla K20c

Host tesla59 using GPU 0: Tesla K20c

Host tesla30 using GPU 0: Tesla K20c

[tesla27:44899] *** Process received signal ***
[tesla27:44899] Signal: Segmentation fault (11)
[tesla27:44899] Signal code: Invalid permissions (2)
[tesla27:44899] Failing at address: 0x7f2e74291ff0
[tesla27:44899] [ 0] /lib64/libpthread.so.0(+0xf100)[0x7f2e81170100]
[tesla27:44899] [ 1] /lib64/libc.so.6(+0x14e92e)[0x7f2e80eed92e]
[tesla27:44899] [ 2] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/libopen-pal.so.13(opal_convertor_pack+0x196)[0x7f2e8087b866]
[tesla27:44899] [ 3] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_btl_openib.so(mca_btl_openib_prepare_src+0xd7)[0x7f2e7b6d64f7]
[tesla27:44899] [ 4] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send_request_schedule_once+0x1ef)[0x7f2e79fe570f]
[tesla27:44899] [ 5] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_pml_ob1.so(+0x14676)[0x7f2e79fe6676]
[tesla27:44899] [ 6] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_pml_ob1.so(+0x16295)[0x7f2e79fe8295]
[tesla27:44899] [ 7] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_btl_openib.so(+0x1340f)[0x7f2e7b6dc40f]
[tesla27:44899] [ 8] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_btl_openib.so(+0x1379c)[0x7f2e7b6dc79c]
[tesla27:44899] [ 9] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/libopen-pal.so.13(opal_progress+0x2a)[0x7f2e8086ee9a]
[tesla27:44899] [10] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_pml_ob1.so(+0xa045)[0x7f2e79fdc045]
[tesla27:44899] [11] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x237)[0x7f2e79fdcf67]
[tesla27:44899] [12] /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/libmpi.so.12(MPI_Sendrecv+0x21a)[0x7f2e81cea83a]
[tesla27:44899] [13] /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi[0x41465d]
[tesla27:44899] [14] /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi[0x40a836]
[tesla27:44899] [15] /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi[0x412ff3]
[tesla27:44899] [16] /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi[0x4028d5]
[tesla27:44899] [17] /lib64/libc.so.6(__libc_start_main+0xf5)[0x7f2e80dc0b15]
[tesla27:44899] [18] /home/hpcagos1/CoMD-CUDA/bin/CoMD-cuda-mpi[0x403b11]
[tesla27:44899] *** End of error message ***
[tesla29:586] *** An error occurred in MPI_Sendrecv
[tesla29:586] *** reported by process [3622436865,2]
[tesla29:586] *** on communicator MPI_COMM_WORLD
[tesla29:586] *** MPI_ERR_TRUNCATE: message truncated
[tesla29:586] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[tesla29:586] ***    and potentially your MPI job)
[tesla27:44862] 5 more processes have sent help message help-mpi-errors.txt / mpi_errors_are_fatal
[tesla27:44862] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
+ date
mer 23 nov 2016, 09.33.52, GMT
