+ local A=0
+ local B=0
+ local C=1
+ local D=1
+ local NP=16
+ shift 5
+ local 'PAR=-e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80'
+ date
mer 23 nov 2016, 11.48.49, GMT
+ tee -a run.log
+ echo

+ echo

+ mpirun -x ASYNC_USE_ASYNC=0 -x ASYNC_ENABLE_DEBUG=0 -x COMM_USE_COMM=1 -x COMM_USE_ASYNC=1 -x COMM_USE_GPU_COMM=0 -x MP_ENABLE_DEBUG=0 -x GDS_ENABLE_DEBUG=0 -x ENABLE_DEBUG_MSG=0 -x MLX5_DEBUG_MASK=0 -x MLX5_FREEZE_ON_ERROR_CQE=0 -x MP_DBREC_ON_GPU=0 -x MP_RX_CQ_ON_GPU=0 -x MP_TX_CQ_ON_GPU=0 -x MP_EVENT_ASYNC=0 -x MP_GUARD_PROGRESS=0 -x GDS_DISABLE_WRITE64=0 -x GDS_SIMULATE_WRITE64=0 -x GDS_DISABLE_INLINECOPY=0 -x GDS_ENABLE_WEAK_CONSISTENCY=1 -x GDS_DISABLE_MEMBAR=1 -x CUDA_VISIBLE_DEVICES=0 -x CUDA_DISABLE_UNIFIED_MEMORY=0 --mca btl_openib_want_cuda_gdr 1 --map-by node -np 16 -mca btl_openib_warn_default_gid_prefix 0 /home/hpcagos1/peersync/src/scripts/wrapper.sh /home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla31:19197] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla27:12584] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla57:04799] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla59:02437] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla29:15010] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla28:24285] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla58:03249] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla30:23715] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla92:18950] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla94:06093] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla93:12394] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla97:44566] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla99:11948] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla96:02746] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla95:43749] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla98:36136] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
Wed Nov 23 11:48:51 2016: Starting Initialization


Mini-Application Name    : CoMD-cuda-mpi
Mini-Application Version : 1.1
Platform:
  hostname: tesla27
  kernel name: 'Linux'
  kernel release: '3.10.0-327.36.3.el7.x86_64'
  processor: 'x86_64'
Build:
  CC: '/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc'
  compiler version: 'gcc (GCC) 5.3.0'
  CFLAGS: '-std=c++11 -Wno-unused-result -DMAXATOMS=64  -DNDEBUG  -DCOMD_DOUBLE -DDO_MPI -DUSE_ASYNC -I/home/hpcagos1/peersync/include  -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/hwloc/hwloc191/hwloc/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi -I/home/hpcagos1/peersync/include -I/usr/local/Cluster-Apps/cuda/8.0/include -D_NVPROF_NVTX -DUSE_ASYNC'
  LDFLAGS: ' -L/home/hpcagos1/peersync/lib -lmp -lgdsync -lgdrapi -lcuda -libverbs  -lnvToolsExt -lm -lstdc++ -L/usr/local/Cluster-Apps/cuda/8.0/lib64 -lcudart '
  using MPI: true
  Threading: none
  Double Precision: true
Run Date/Time: 2016-11-23, 11:48:51

Command Line Parameters:
  doeam: 1
  potDir: pots
  potName: Cu_u6.eam
  potType: funcfl
  nx: 80
  ny: 80
  nz: 80
  xproc: 4
  yproc: 2
  zproc: 2
  Lattice constant: -1 Angstroms
  nSteps: 100
  printRate: 10
  Time step: 1 fs
  Initial Temperature: 600 K
  Initial Delta: 0 Angstroms

  GPU async opt: 0
  GPU profiling mode: 0
  GPU method: thread_atom
  Space-filling (Hilbert): 0

Host tesla27 using GPU 0: Tesla K20c

RANK[0]: useNL:0,  sim->method:0
Host tesla28 using GPU 0: Tesla K20c

RANK[1]: useNL:0,  sim->method:0
Host tesla29 using GPU 0: Tesla K20c

RANK[2]: useNL:0,  sim->method:0
Host tesla57 using GPU 0: Tesla K20c

RANK[5]: useNL:0,  sim->method:0
Host tesla59 using GPU 0: Tesla K20c

RANK[7]: useNL:0,  sim->method:0
Host tesla31 using GPU 0: Tesla K20c

RANK[4]: useNL:0,  sim->method:0
Host tesla58 using GPU 0: Tesla K20c

RANK[6]: useNL:0,  sim->method:0
Host tesla30 using GPU 0: Tesla K20c

RANK[3]: useNL:0,  sim->method:0
Host tesla97 using GPU 0: Tesla K20c

RANK[13]: useNL:0,  sim->method:0
Host tesla93 using GPU 0: Tesla K20c

RANK[9]: useNL:0,  sim->method:0
Host tesla95 using GPU 0: Tesla K20c

RANK[11]: useNL:0,  sim->method:0
Host tesla96 using GPU 0: Tesla K20c

RANK[12]: useNL:0,  sim->method:0
Host tesla92 using GPU 0: Tesla K20c

RANK[8]: useNL:0,  sim->method:0
Host tesla98 using GPU 0: Tesla K20c

RANK[14]: useNL:0,  sim->method:0
Host tesla99 using GPU 0: Tesla K20c

RANK[15]: useNL:0,  sim->method:0
Host tesla94 using GPU 0: Tesla K20c

RANK[10]: useNL:0,  sim->method:0
[12584] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[12584] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[12584] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[12584] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[44566] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[2437] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[19197] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[3249] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[18950] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[12394] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[24285] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[23715] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[11948] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[2746] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[15010] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[4799] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[36136] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[15010] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[4799] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[36136] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[44566] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[2437] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[19197] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[3249] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[18950] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[12394] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[6093] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[24285] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[23715] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[11948] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[2746] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[36136] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[44566] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[2437] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[19197] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[3249] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[18950] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[12394] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[6093] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[24285] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[23715] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[11948] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[2746] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[43749] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[15010] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[4799] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[11948] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[2746] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[43749] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[15010] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[4799] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[36136] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[44566] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[2437] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[19197] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[3249] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[18950] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[12394] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[6093] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[24285] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[23715] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[3249] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[6093] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[24285] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[43749] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[36136] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[12584] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[43749] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[19197] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[11948] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[2746] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[15010] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[44566] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[2437] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[18950] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[12394] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[6093] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[23715] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[4799] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[43749] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
Simulation data: 
  Total atoms        : 2048000
  Min global bounds  : [   0.0000000000,   0.0000000000,   0.0000000000 ]
  Max global bounds  : [ 289.2000000000, 289.2000000000, 289.2000000000 ]

Decomposition data: 
  Processors         :      4,     2,     2
  Local boxes        :     14,    29,    29 =    11774
  Box size           : [   5.1642857143,   4.9862068966,   4.9862068966 ]
  Box factor         : [   1.0432900433,   1.0073145246,   1.0073145246 ] 
  Max Link Cell Occupancy: 14 of 64

Potential data: 
  Potential type  : EAM
  Species name    : Cu
  Atomic number   : 29
  Mass            : 63.55 amu
  Lattice type    : FCC
  Lattice spacing : 3.615 Angstroms
  Cutoff          : 4.95 Angstroms

Memory data: 
  Intrinsic atom footprint =   88 B/atom 
  Total atom footprint     = 171.875 MB ( 10.74 MB/node)
  Link cell atom footprint =  63.239 MB/node
  Link cell atom footprint =  82.586 MB/node (including halo cell data


Initial energy : -3.460523233086, atom count : 2048000 

Wed Nov 23 11:48:52 2016: Initialization Finished

Wed Nov 23 11:48:52 2016: Starting simulation

#                                                                                         Performance
#  Loop   Time(fs)       Total Energy   Potential Energy     Kinetic Energy  Temperature   (us/atom)     # Atoms
      0       0.00    -3.460523233086    -3.538079224686     0.077555991600     600.0000     0.0000      2048000
     10      10.00    -3.460520828058    -3.529930504519     0.069409676461     536.9773     0.1309      2048000
     20      20.00    -3.460517972527    -3.509741553170     0.049223580643     380.8107     0.1302      2048000
     30      30.00    -3.460516752964    -3.488525259584     0.028008506621     216.6835     0.1304      2048000
     40      40.00    -3.460518405249    -3.477509965770     0.016991560521     131.4526     0.1309      2048000
     50      50.00    -3.460522801259    -3.479755986413     0.019233185154     148.7946     0.1312      2048000
     60      60.00    -3.460526484106    -3.488945347810     0.028418863704     219.8582     0.1312      2048000
     70      70.00    -3.460527199066    -3.496663074098     0.036135875032     279.5596     0.1315      2048000
     80      80.00    -3.460525474869    -3.498977977596     0.038452502727     297.4819     0.1314      2048000
     90      90.00    -3.460522714197    -3.497375316040     0.036852601843     285.1045     0.1316      2048000
    100     100.00    -3.460520179098    -3.495869590110     0.035349411012     273.4753     0.1317      2048000
Wed Nov 23 11:48:54 2016: Ending simulation



Simulation Validation:
  Initial energy  : -3.460523233086
  Final energy    : -3.460520179098
  eFinal/eInitial : 0.999999
  Final atom count : 2048000, no atoms lost


Timings for Rank 0
        Timer        # Calls    Avg/Call (s)   Total (s)    % Loop
___________________________________________________________________
total                      1       2.1463        2.1463      127.83
loop                       1       1.6791        1.6791      100.00
timestep                  10       0.1678        1.6782       99.95
  position               100       0.0000        0.0005        0.03
  velocity               200       0.0000        0.0011        0.07
  redistribute           101       0.0142        1.4331       85.35
    atomHalo             101       0.0005        0.0552        3.29
  force                  101       0.0013        0.1263        7.52
    eamHalo              101       0.0012        0.1242        7.40
commHalo                 202       0.0003        0.0653        3.89
commReduce                39       0.0009        0.0358        2.13

Timing Statistics Across 16 Ranks:
        Timer        Rank: Min(s)       Rank: Max(s)      Avg(s)    Stdev(s)
_____________________________________________________________________________
total                1:    2.1463       0:    2.1463      2.1463      0.0000
loop                 5:    1.6790      10:    1.6793      1.6792      0.0001
timestep             0:    1.6782      12:    1.6783      1.6783      0.0000
  position           3:    0.0005       4:    0.0005      0.0005      0.0000
  velocity           3:    0.0011       2:    0.0011      0.0011      0.0000
  redistribute      11:    1.4262       3:    1.4357      1.4319      0.0019
    atomHalo        11:    0.0492       1:    0.0567      0.0545      0.0016
  force              3:    0.1229       8:    0.1297      0.1267      0.0013
    eamHalo          3:    0.1209       8:    0.1277      0.1246      0.0013
commHalo             3:    0.0644      11:    0.0668      0.0654      0.0006
commReduce          10:    0.0034       1:    0.0376      0.0151      0.0141

---------------------------------------------------
 Average atom update rate:       0.13 us/atom/task
---------------------------------------------------


---------------------------------------------------
 Average all atom update rate:   0.01 us/atom
---------------------------------------------------


---------------------------------------------------
 Average atom rate:            122.03 atoms/us
---------------------------------------------------

Wed Nov 23 11:48:54 2016: CoMD Ending

/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
+ date
mer 23 nov 2016, 11.48.54, GMT
