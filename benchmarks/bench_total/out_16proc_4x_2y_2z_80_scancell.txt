+ echo CWD=/home/hpcagos1/peersync/src/comd-cuda-async
CWD=/home/hpcagos1/peersync/src/comd-cuda-async
+ run 0 0 0 1 16 -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80
+ local A=0
+ local B=0
+ local C=0
+ local D=1
+ local NP=16
+ shift 5
+ local 'PAR=-e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80'
+ date
mer 23 nov 2016, 11.01.28, GMT
+ tee -a run.log
+ echo

+ echo

+ mpirun -x ASYNC_USE_ASYNC=0 -x ASYNC_ENABLE_DEBUG=0 -x COMM_USE_COMM=1 -x COMM_USE_ASYNC=1 -x COMM_USE_GPU_COMM=0 -x MP_ENABLE_DEBUG=0 -x GDS_ENABLE_DEBUG=0 -x ENABLE_DEBUG_MSG=0 -x MLX5_DEBUG_MASK=0 -x MLX5_FREEZE_ON_ERROR_CQE=0 -x MP_DBREC_ON_GPU=0 -x MP_RX_CQ_ON_GPU=0 -x MP_TX_CQ_ON_GPU=0 -x MP_EVENT_ASYNC=0 -x MP_GUARD_PROGRESS=0 -x GDS_DISABLE_WRITE64=0 -x GDS_SIMULATE_WRITE64=0 -x GDS_DISABLE_INLINECOPY=0 -x GDS_ENABLE_WEAK_CONSISTENCY=0 -x GDS_DISABLE_MEMBAR=1 -x CUDA_VISIBLE_DEVICES=0 -x CUDA_DISABLE_UNIFIED_MEMORY=0 --mca btl_openib_want_cuda_gdr 1 --map-by node -np 16 -mca btl_openib_warn_default_gid_prefix 0 /home/hpcagos1/peersync/src/scripts/wrapper.sh /home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla27:07061] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla30:15700] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla94:47716] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla99:05349] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla97:37938] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla58:45271] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla31:12253] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla98:29299] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla59:44162] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla93:05518] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla96:44777] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla95:37129] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla92:12246] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla57:46617] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla29:08735] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla28:18367] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
Wed Nov 23 11:01:29 2016: Starting Initialization


Mini-Application Name    : CoMD-cuda-mpi
Mini-Application Version : 1.1
Platform:
  hostname: tesla27
  kernel name: 'Linux'
  kernel release: '3.10.0-327.36.3.el7.x86_64'
  processor: 'x86_64'
Build:
  CC: '/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc'
  compiler version: 'gcc (GCC) 5.3.0'
  CFLAGS: '-std=c++11 -Wno-unused-result -DMAXATOMS=64  -DNDEBUG  -DCOMD_DOUBLE -DDO_MPI -DUSE_ASYNC -I/home/hpcagos1/peersync/include  -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/hwloc/hwloc191/hwloc/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi -I/home/hpcagos1/peersync/include -I/usr/local/Cluster-Apps/cuda/8.0/include -D_NVPROF_NVTX -DUSE_ASYNC'
  LDFLAGS: ' -L/home/hpcagos1/peersync/lib -lmp -lgdsync -lgdrapi -lcuda -libverbs  -lnvToolsExt -lm -lstdc++ -L/usr/local/Cluster-Apps/cuda/8.0/lib64 -lcudart '
  using MPI: true
  Threading: none
  Double Precision: true
Run Date/Time: 2016-11-23, 11:01:29

Command Line Parameters:
  doeam: 1
  potDir: pots
  potName: Cu_u6.eam
  potType: funcfl
  nx: 80
  ny: 80
  nz: 80
  xproc: 4
  yproc: 2
  zproc: 2
  Lattice constant: -1 Angstroms
  nSteps: 100
  printRate: 10
  Time step: 1 fs
  Initial Temperature: 600 K
  Initial Delta: 0 Angstroms

  GPU async opt: 0
  GPU profiling mode: 0
  GPU method: thread_atom
  Space-filling (Hilbert): 0

Host tesla27 using GPU 0: Tesla K20c

RANK[0]: useNL:0,  sim->method:0
Host tesla96 using GPU 0: Tesla K20c

RANK[12]: useNL:0,  sim->method:0
Host tesla30 using GPU 0: Tesla K20c

RANK[3]: useNL:0,  sim->method:0
Host tesla28 using GPU 0: Tesla K20c

RANK[1]: useNL:0,  sim->method:0
Host tesla57 using GPU 0: Tesla K20c

RANK[5]: useNL:0,  sim->method:0
Host tesla99 using GPU 0: Tesla K20c

RANK[15]: useNL:0,  sim->method:0
Host tesla31 using GPU 0: Tesla K20c

RANK[4]: useNL:0,  sim->method:0
Host tesla95 using GPU 0: Tesla K20c

RANK[11]: useNL:0,  sim->method:0
Host tesla29 using GPU 0: Tesla K20c

RANK[2]: useNL:0,  sim->method:0
Host tesla94 using GPU 0: Tesla K20c

RANK[10]: useNL:0,  sim->method:0
Host tesla58 using GPU 0: Tesla K20c

RANK[6]: useNL:0,  sim->method:0
Host tesla97 using GPU 0: Tesla K20c

RANK[13]: useNL:0,  sim->method:0
Host tesla59 using GPU 0: Tesla K20c

RANK[7]: useNL:0,  sim->method:0
Host tesla93 using GPU 0: Tesla K20c

RANK[9]: useNL:0,  sim->method:0
Host tesla92 using GPU 0: Tesla K20c

RANK[8]: useNL:0,  sim->method:0
Host tesla98 using GPU 0: Tesla K20c

RANK[14]: useNL:0,  sim->method:0
[7061] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[7061] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[7061] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[7061] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[5518] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[46617] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[45271] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[44777] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[18367] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[15700] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[47716] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[29299] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[5349] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[44162] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[12253] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[37938] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[37129] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[12246] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[8735] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[15700] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[47716] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[29299] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[5349] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[44162] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[12253] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[37938] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[37129] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[12246] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[8735] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[5518] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[46617] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[45271] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[44777] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[18367] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[37129] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[12246] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[8735] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[5518] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[46617] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[45271] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[44777] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[18367] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[15700] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[47716] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[29299] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[5349] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[44162] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[12253] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[37938] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[5518] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[46617] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[45271] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[44777] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[18367] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[15700] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[47716] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[29299] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[5349] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[44162] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[12253] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[37938] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[37129] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[12246] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[8735] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[47716] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[29299] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[45271] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[18367] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[7061] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[12246] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[44777] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[12253] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[37938] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[8735] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[15700] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[5349] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[44162] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[37129] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[5518] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[46617] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
Simulation data: 
  Total atoms        : 2048000
  Min global bounds  : [   0.0000000000,   0.0000000000,   0.0000000000 ]
  Max global bounds  : [ 289.2000000000, 289.2000000000, 289.2000000000 ]

Decomposition data: 
  Processors         :      4,     2,     2
  Local boxes        :     14,    29,    29 =    11774
  Box size           : [   5.1642857143,   4.9862068966,   4.9862068966 ]
  Box factor         : [   1.0432900433,   1.0073145246,   1.0073145246 ] 
  Max Link Cell Occupancy: 14 of 64

Potential data: 
  Potential type  : EAM
  Species name    : Cu
  Atomic number   : 29
  Mass            : 63.55 amu
  Lattice type    : FCC
  Lattice spacing : 3.615 Angstroms
  Cutoff          : 4.95 Angstroms

Memory data: 
  Intrinsic atom footprint =   88 B/atom 
  Total atom footprint     = 171.875 MB ( 10.74 MB/node)
  Link cell atom footprint =  63.239 MB/node
  Link cell atom footprint =  82.586 MB/node (including halo cell data


Initial energy : -3.460523233086, atom count : 2048000 

Wed Nov 23 11:01:30 2016: Initialization Finished

Wed Nov 23 11:01:30 2016: Starting simulation

#                                                                                         Performance
#  Loop   Time(fs)       Total Energy   Potential Energy     Kinetic Energy  Temperature   (us/atom)     # Atoms
      0       0.00    -3.460523233086    -3.538079224686     0.077555991600     600.0000     0.0000      2048000
     10      10.00    -3.460520820422    -3.529930504327     0.069409683904     536.9773     0.1303      2048000
     20      20.00    -3.460517928678    -3.509741549151     0.049223620473     380.8110     0.1303      2048000
     30      30.00    -3.460516680362    -3.488525240126     0.028008559764     216.6839     0.1308      2048000
     40      40.00    -3.460518280980    -3.477509895898     0.016991614919     131.4530     0.1312      2048000
     50      50.00    -3.460522679040    -3.479755815297     0.019233136257     148.7942     0.1315      2048000
     60      60.00    -3.460526365308    -3.488945103703     0.028418738396     219.8572     0.1315      2048000
     70      70.00    -3.460527135651    -3.496662829111     0.036135693460     279.5582     0.1319      2048000
     80      80.00    -3.460525416546    -3.498977763425     0.038452346879     297.4807     0.1317      2048000
     90      90.00    -3.460522630532    -3.497375134245     0.036852503713     285.1037     0.1318      2048000
    100     100.00    -3.460520077218    -3.495869402691     0.035349325473     273.4746     0.1319      2048000
Wed Nov 23 11:01:31 2016: Ending simulation



Simulation Validation:
  Initial energy  : -3.460523233086
  Final energy    : -3.460520077218
  eFinal/eInitial : 0.999999
  Final atom count : 2048000, no atoms lost


Timings for Rank 0
        Timer        # Calls    Avg/Call (s)   Total (s)    % Loop
___________________________________________________________________
total                      1       2.1479        2.1479      127.72
loop                       1       1.6816        1.6816      100.00
timestep                  10       0.1681        1.6805       99.93
  position               100       0.0000        0.0005        0.03
  velocity               200       0.0000        0.0011        0.06
  redistribute           101       0.0142        1.4318       85.14
    atomHalo             101       0.0005        0.0493        2.93
  force                  101       0.0012        0.1229        7.31
    eamHalo              101       0.0012        0.1210        7.19
commHalo                 202       0.0003        0.0646        3.84
commReduce                39       0.0001        0.0057        0.34

Timing Statistics Across 16 Ranks:
        Timer        Rank: Min(s)       Rank: Max(s)      Avg(s)    Stdev(s)
_____________________________________________________________________________
total                5:    2.1478       0:    2.1479      2.1478      0.0000
loop                 5:    1.6814       4:    1.6818      1.6816      0.0001
timestep             0:    1.6805       2:    1.6807      1.6806      0.0000
  position           4:    0.0005      15:    0.0005      0.0005      0.0000
  velocity          14:    0.0011      10:    0.0011      0.0011      0.0000
  redistribute      11:    1.4274       3:    1.4330      1.4308      0.0015
    atomHalo         5:    0.0480       1:    0.0511      0.0497      0.0006
  force              3:    0.1211      11:    0.1271      0.1236      0.0016
    eamHalo          3:    0.1191      11:    0.1251      0.1216      0.0016
commHalo             9:    0.0640      15:    0.0655      0.0648      0.0004
commReduce           3:    0.0025       0:    0.0057      0.0040      0.0007

---------------------------------------------------
 Average atom update rate:       0.13 us/atom/task
---------------------------------------------------


---------------------------------------------------
 Average all atom update rate:   0.01 us/atom
---------------------------------------------------


---------------------------------------------------
 Average atom rate:            121.86 atoms/us
---------------------------------------------------

Wed Nov 23 11:01:31 2016: CoMD Ending

/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
+ date
mer 23 nov 2016, 11.01.32, GMT
