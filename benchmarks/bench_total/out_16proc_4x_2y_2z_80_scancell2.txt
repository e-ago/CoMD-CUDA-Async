+ local A=0
+ local B=0
+ local C=0
+ local D=1
+ local NP=16
+ shift 5
+ local 'PAR=-e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80'
+ date
mer 23 nov 2016, 11.10.33, GMT
+ tee -a run.log
+ echo

+ echo

+ mpirun -x ASYNC_USE_ASYNC=0 -x ASYNC_ENABLE_DEBUG=0 -x COMM_USE_COMM=1 -x COMM_USE_ASYNC=1 -x COMM_USE_GPU_COMM=0 -x MP_ENABLE_DEBUG=0 -x GDS_ENABLE_DEBUG=0 -x ENABLE_DEBUG_MSG=0 -x MLX5_DEBUG_MASK=0 -x MLX5_FREEZE_ON_ERROR_CQE=0 -x MP_DBREC_ON_GPU=0 -x MP_RX_CQ_ON_GPU=0 -x MP_TX_CQ_ON_GPU=0 -x MP_EVENT_ASYNC=0 -x MP_GUARD_PROGRESS=0 -x GDS_DISABLE_WRITE64=0 -x GDS_SIMULATE_WRITE64=0 -x GDS_DISABLE_INLINECOPY=0 -x GDS_ENABLE_WEAK_CONSISTENCY=0 -x GDS_DISABLE_MEMBAR=1 -x CUDA_VISIBLE_DEVICES=0 -x CUDA_DISABLE_UNIFIED_MEMORY=0 --mca btl_openib_want_cuda_gdr 1 --map-by node -np 16 -mca btl_openib_warn_default_gid_prefix 0 /home/hpcagos1/peersync/src/scripts/wrapper.sh /home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla31:14105] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla27:08593] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla29:10375] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla57:48447] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla58:47129] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla28:19942] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla59:46000] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla30:17899] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla92:14089] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla94:00799] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla98:31059] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla95:38931] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla99:07126] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla96:46603] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla93:07312] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla97:39709] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
Wed Nov 23 11:10:36 2016: Starting Initialization


Mini-Application Name    : CoMD-cuda-mpi
Mini-Application Version : 1.1
Platform:
  hostname: tesla27
  kernel name: 'Linux'
  kernel release: '3.10.0-327.36.3.el7.x86_64'
  processor: 'x86_64'
Build:
  CC: '/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc'
  compiler version: 'gcc (GCC) 5.3.0'
  CFLAGS: '-std=c++11 -Wno-unused-result -DMAXATOMS=64  -DNDEBUG  -DCOMD_DOUBLE -DDO_MPI -DUSE_ASYNC -I/home/hpcagos1/peersync/include  -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/hwloc/hwloc191/hwloc/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi -I/home/hpcagos1/peersync/include -I/usr/local/Cluster-Apps/cuda/8.0/include -D_NVPROF_NVTX -DUSE_ASYNC'
  LDFLAGS: ' -L/home/hpcagos1/peersync/lib -lmp -lgdsync -lgdrapi -lcuda -libverbs  -lnvToolsExt -lm -lstdc++ -L/usr/local/Cluster-Apps/cuda/8.0/lib64 -lcudart '
  using MPI: true
  Threading: none
  Double Precision: true
Run Date/Time: 2016-11-23, 11:10:36

Command Line Parameters:
  doeam: 1
  potDir: pots
  potName: Cu_u6.eam
  potType: funcfl
  nx: 80
  ny: 80
  nz: 80
  xproc: 4
  yproc: 2
  zproc: 2
  Lattice constant: -1 Angstroms
  nSteps: 100
  printRate: 10
  Time step: 1 fs
  Initial Temperature: 600 K
  Initial Delta: 0 Angstroms

  GPU async opt: 0
  GPU profiling mode: 0
  GPU method: thread_atom
  Space-filling (Hilbert): 0

Host tesla27 using GPU 0: Tesla K20c

RANK[0]: useNL:0,  sim->method:0
Host tesla29 using GPU 0: Tesla K20c

RANK[2]: useNL:0,  sim->method:0
Host tesla31 using GPU 0: Tesla K20c

RANK[4]: useNL:0,  sim->method:0
Host tesla58 using GPU 0: Tesla K20c

RANK[6]: useNL:0,  sim->method:0
Host tesla28 using GPU 0: Tesla K20c

RANK[1]: useNL:0,  sim->method:0
Host tesla57 using GPU 0: Tesla K20c

RANK[5]: useNL:0,  sim->method:0
Host tesla59 using GPU 0: Tesla K20c

RANK[7]: useNL:0,  sim->method:0
Host tesla30 using GPU 0: Tesla K20c

RANK[3]: useNL:0,  sim->method:0
Host tesla93 using GPU 0: Tesla K20c

RANK[9]: useNL:0,  sim->method:0
Host tesla96 using GPU 0: Tesla K20c

RANK[12]: useNL:0,  sim->method:0
Host tesla94 using GPU 0: Tesla K20c

RANK[10]: useNL:0,  sim->method:0
Host tesla92 using GPU 0: Tesla K20c

RANK[8]: useNL:0,  sim->method:0
Host tesla99 using GPU 0: Tesla K20c

RANK[15]: useNL:0,  sim->method:0
Host tesla97 using GPU 0: Tesla K20c

RANK[13]: useNL:0,  sim->method:0
Host tesla95 using GPU 0: Tesla K20c

RANK[11]: useNL:0,  sim->method:0
Host tesla98 using GPU 0: Tesla K20c

RANK[14]: useNL:0,  sim->method:0
[8593] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[8593] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[46000] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[39709] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[799] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[47129] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[46000] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[10375] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[19942] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[48447] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[46603] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[17899] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[14105] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[8593] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[8593] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[39709] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[799] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[47129] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[46000] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[10375] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[19942] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[48447] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[46603] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[17899] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[14105] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[799] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[47129] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[46000] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[10375] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[19942] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[38931] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[48447] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[46603] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[17899] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[31059] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[14105] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[7312] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[39709] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[31059] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[14105] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[7312] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[7126] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[39709] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[799] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[47129] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[10375] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[19942] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[38931] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[48447] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[46603] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[17899] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[38931] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[31059] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[7312] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[7126] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[38931] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[31059] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[7312] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[7126] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[7126] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[14089] GDS INFO  gds_enable_write64() GDS_DISABLE_WRITE64=0
[14089] GDS INFO  gds_enable_inlcpy() GDS_DISABLE_INLINECOPY=0
[14089] GDS INFO  gds_simulate_write64() GDS_SIMULATE_WRITE64=0
[14089] GDS INFO  gds_enable_membar() GDS_DISABLE_MEMBAR=1
[799] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[19942] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[47129] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[14105] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[46000] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[39709] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[10375] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[17899] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[48447] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[46603] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[8593] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[7126] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[31059] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[38931] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[7312] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[14089] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
Simulation data: 
  Total atoms        : 2048000
  Min global bounds  : [   0.0000000000,   0.0000000000,   0.0000000000 ]
  Max global bounds  : [ 289.2000000000, 289.2000000000, 289.2000000000 ]

Decomposition data: 
  Processors         :      4,     2,     2
  Local boxes        :     14,    29,    29 =    11774
  Box size           : [   5.1642857143,   4.9862068966,   4.9862068966 ]
  Box factor         : [   1.0432900433,   1.0073145246,   1.0073145246 ] 
  Max Link Cell Occupancy: 14 of 64

Potential data: 
  Potential type  : EAM
  Species name    : Cu
  Atomic number   : 29
  Mass            : 63.55 amu
  Lattice type    : FCC
  Lattice spacing : 3.615 Angstroms
  Cutoff          : 4.95 Angstroms

Memory data: 
  Intrinsic atom footprint =   88 B/atom 
  Total atom footprint     = 171.875 MB ( 10.74 MB/node)
  Link cell atom footprint =  63.239 MB/node
  Link cell atom footprint =  82.586 MB/node (including halo cell data


Initial energy : -3.460523233086, atom count : 2048000 

Wed Nov 23 11:10:36 2016: Initialization Finished

Wed Nov 23 11:10:36 2016: Starting simulation

#                                                                                         Performance
#  Loop   Time(fs)       Total Energy   Potential Energy     Kinetic Energy  Temperature   (us/atom)     # Atoms
      0       0.00    -3.460523233086    -3.538079224686     0.077555991600     600.0000     0.0000      2048000
     10      10.00    -3.460520836458    -3.529930505071     0.069409668613     536.9772     0.1317      2048000
     20      20.00    -3.460517960794    -3.509741553481     0.049223592687     380.8108     0.1301      2048000
     30      30.00    -3.460516722626    -3.488525241289     0.028008518663     216.6836     0.1305      2048000
     40      40.00    -3.460518380353    -3.477509915385     0.016991535032     131.4524     0.1310      2048000
     50      50.00    -3.460522789942    -3.479755925580     0.019233135639     148.7942     0.1312      2048000
     60      60.00    -3.460526480158    -3.488945313122     0.028418832964     219.8579     0.1313      2048000
     70      70.00    -3.460527182857    -3.496663058443     0.036135875586     279.5596     0.1318      2048000
     80      80.00    -3.460525469346    -3.498977932430     0.038452463085     297.4816     0.1316      2048000
     90      90.00    -3.460522711878    -3.497375204477     0.036852492600     285.1036     0.1316      2048000
    100     100.00    -3.460520220990    -3.495869424345     0.035349203355     273.4737     0.1318      2048000
Wed Nov 23 11:10:38 2016: Ending simulation



Simulation Validation:
  Initial energy  : -3.460523233086
  Final energy    : -3.460520220990
  eFinal/eInitial : 0.999999
  Final atom count : 2048000, no atoms lost


Timings for Rank 0
        Timer        # Calls    Avg/Call (s)   Total (s)    % Loop
___________________________________________________________________
total                      1       2.1504        2.1504      127.92
loop                       1       1.6810        1.6810      100.00
timestep                  10       0.1680        1.6802       99.95
  position               100       0.0000        0.0005        0.03
  velocity               200       0.0000        0.0011        0.06
  redistribute           101       0.0142        1.4348       85.35
    atomHalo             101       0.0006        0.0565        3.36
  force                  101       0.0012        0.1258        7.48
    eamHalo              101       0.0012        0.1238        7.37
commHalo                 202       0.0003        0.0652        3.88
commReduce                39       0.0009        0.0359        2.14

Timing Statistics Across 16 Ranks:
        Timer        Rank: Min(s)       Rank: Max(s)      Avg(s)    Stdev(s)
_____________________________________________________________________________
total                5:    2.1504       0:    2.1504      2.1504      0.0000
loop                 1:    1.6809      14:    1.6813      1.6811      0.0001
timestep             0:    1.6802      12:    1.6803      1.6803      0.0000
  position           9:    0.0005       8:    0.0006      0.0005      0.0000
  velocity           3:    0.0011       4:    0.0011      0.0011      0.0000
  redistribute      14:    1.4256       9:    1.4368      1.4332      0.0028
    atomHalo        14:    0.0496      12:    0.0580      0.0553      0.0022
  force              9:    0.1243       5:    0.1295      0.1266      0.0017
    eamHalo         10:    0.1223       5:    0.1275      0.1246      0.0017
commHalo             6:    0.0649      14:    0.0674      0.0659      0.0006
commReduce          14:    0.0033       0:    0.0359      0.0146      0.0139

---------------------------------------------------
 Average atom update rate:       0.13 us/atom/task
---------------------------------------------------


---------------------------------------------------
 Average all atom update rate:   0.01 us/atom
---------------------------------------------------


---------------------------------------------------
 Average atom rate:            121.88 atoms/us
---------------------------------------------------

Wed Nov 23 11:10:38 2016: CoMD Ending

/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 4 -j 2 -k 2 -x 80 -y 80 -z 80  
+ date
mer 23 nov 2016, 11.10.38, GMT
