+ local A=0
+ local B=0
+ local C=1
+ local D=1
+ local NP=16
+ shift 5
+ local 'PAR=-e -i 16 -j 1 -k 1 -x 80 -y 80 -z 80'
+ date
mer 23 nov 2016, 09.38.32, GMT
+ tee -a run.log
+ echo

+ echo

+ mpirun -x ASYNC_USE_ASYNC=0 -x ASYNC_ENABLE_DEBUG=0 -x COMM_USE_COMM=1 -x COMM_USE_ASYNC=1 -x COMM_USE_GPU_COMM=0 -x MP_ENABLE_DEBUG=0 -x GDS_ENABLE_DEBUG=0 -x ENABLE_DEBUG_MSG=0 -x MLX5_DEBUG_MASK=0 -x MLX5_FREEZE_ON_ERROR_CQE=0 -x MP_DBREC_ON_GPU=0 -x MP_RX_CQ_ON_GPU=0 -x MP_TX_CQ_ON_GPU=0 -x MP_EVENT_ASYNC=0 -x MP_GUARD_PROGRESS=0 -x GDS_DISABLE_WRITE64=0 -x GDS_SIMULATE_WRITE64=0 -x GDS_DISABLE_INLINECOPY=0 -x GDS_ENABLE_WEAK_CONSISTENCY=1 -x GDS_DISABLE_MEMBAR=1 -x CUDA_VISIBLE_DEVICES=0 -x CUDA_DISABLE_UNIFIED_MEMORY=0 --mca btl_openib_want_cuda_gdr 1 --map-by node -np 16 -mca btl_openib_warn_default_gid_prefix 0 /home/hpcagos1/peersync/src/scripts/wrapper.sh /home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 16 -j 1 -k 1 -x 80 -y 80 -z 80
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
# tesla27: picking GPU:0/ CPU: HCA:
[tesla27:45488] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla92:05344] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla31:05592] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla29:01475] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla94:40757] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla93:47704] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla30:07396] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla96:38018] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla58:38319] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla57:39783] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla59:37356] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla97:31254] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla98:22569] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla28:11458] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla95:30139] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
[tesla99:47413] mca: base: component_find: unable to open /usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib/openmpi/mca_mtl_psm: libpsm_infinipath.so.1: cannot open shared object file: No such file or directory (ignored)
Wed Nov 23 09:38:33 2016: Starting Initialization


Mini-Application Name    : CoMD-cuda-mpi
Mini-Application Version : 1.1
Platform:
  hostname: tesla27
  kernel name: 'Linux'
  kernel release: '3.10.0-327.36.3.el7.x86_64'
  processor: 'x86_64'
Build:
  CC: '/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc'
  compiler version: 'gcc (GCC) 5.3.0'
  CFLAGS: '-std=c++11 -Wno-unused-result -DMAXATOMS=64  -DNDEBUG  -DCOMD_DOUBLE -DDO_MPI -DUSE_ASYNC -I/home/hpcagos1/peersync/include  -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/hwloc/hwloc191/hwloc/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi/opal/mca/event/libevent2021/libevent/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/include/openmpi -I/home/hpcagos1/peersync/include -I/usr/local/Cluster-Apps/cuda/8.0/include -D_NVPROF_NVTX -DUSE_ASYNC'
  LDFLAGS: ' -L/home/hpcagos1/peersync/lib -lmp -lgdsync -lgdrapi -lcuda -libverbs  -lnvToolsExt -lm -lstdc++ -L/usr/local/Cluster-Apps/cuda/8.0/lib64 -lcudart '
  using MPI: true
  Threading: none
  Double Precision: true
Run Date/Time: 2016-11-23, 09:38:33

Command Line Parameters:
  doeam: 1
  potDir: pots
  potName: Cu_u6.eam
  potType: funcfl
  nx: 80
  ny: 80
  nz: 80
  xproc: 16
  yproc: 1
  zproc: 1
  Lattice constant: -1 Angstroms
  nSteps: 100
  printRate: 10
  Time step: 1 fs
  Initial Temperature: 600 K
  Initial Delta: 0 Angstroms

  GPU async opt: 0
  GPU profiling mode: 0
  GPU method: thread_atom
  Space-filling (Hilbert): 0

Host tesla58 using GPU 0: Tesla K20c

RANK[6]: useNL:0,  sim->method:0
Host tesla93 using GPU 0: Tesla K20c

RANK[9]: useNL:0,  sim->method:0
Host tesla27 using GPU 0: Tesla K20c

RANK[0]: useNL:0,  sim->method:0
Host tesla31 using GPU 0: Tesla K20c

RANK[4]: useNL:0,  sim->method:0
Host tesla99 using GPU 0: Tesla K20c

RANK[15]: useNL:0,  sim->method:0
Host tesla97 using GPU 0: Tesla K20c

RANK[13]: useNL:0,  sim->method:0
Host tesla29 using GPU 0: Tesla K20c

RANK[2]: useNL:0,  sim->method:0
Host tesla57 using GPU 0: Tesla K20c

RANK[5]: useNL:0,  sim->method:0
Host tesla30 using GPU 0: Tesla K20c

RANK[3]: useNL:0,  sim->method:0
Host tesla92 using GPU 0: Tesla K20c

RANK[8]: useNL:0,  sim->method:0
Host tesla96 using GPU 0: Tesla K20c

RANK[12]: useNL:0,  sim->method:0
Host tesla94 using GPU 0: Tesla K20c

RANK[10]: useNL:0,  sim->method:0
Host tesla59 using GPU 0: Tesla K20c

RANK[7]: useNL:0,  sim->method:0
Host tesla98 using GPU 0: Tesla K20c

RANK[14]: useNL:0,  sim->method:0
Host tesla95 using GPU 0: Tesla K20c

RANK[11]: useNL:0,  sim->method:0
Host tesla28 using GPU 0: Tesla K20c

RANK[1]: useNL:0,  sim->method:0
ERR [5] comm_isend_on_stream() SIZE==0
ERR [11] comm_isend_on_stream() SIZE==0
ERR [0] comm_isend_on_stream() SIZE==0
ERR [13] comm_isend_on_stream() SIZE==0
ERR [6] comm_isend_on_stream() SIZE==0
ERR [1] comm_isend_on_stream() SIZE==0
ERR [12] comm_isend_on_stream() SIZE==0
ERR [9] comm_isend_on_stream() SIZE==0
ERR [7] comm_isend_on_stream() SIZE==0
ERR [2] comm_isend_on_stream() SIZE==0
ERR [15] comm_isend_on_stream() SIZE==0
ERR [3] comm_isend_on_stream() SIZE==0
ERR [8] comm_isend_on_stream() SIZE==0
ERR [14] comm_isend_on_stream() SIZE==0
ERR [10] comm_isend_on_stream() SIZE==0
ERR [4] comm_isend_on_stream() SIZE==0
ERR [5] comm_isend_on_stream() SIZE==0
ERR [11] comm_isend_on_stream() SIZE==0
ERR [3] comm_isend_on_stream() SIZE==0
ERR [2] comm_isend_on_stream() SIZE==0
ERR [13] comm_isend_on_stream() SIZE==0
ERR [6] comm_isend_on_stream() SIZE==0
[39783] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[30139] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[38319] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [1] comm_isend_on_stream() SIZE==0
ERR [12] comm_isend_on_stream() SIZE==0
[7396] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [9] comm_isend_on_stream() SIZE==0
ERR [7] comm_isend_on_stream() SIZE==0
[31254] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [0] comm_isend_on_stream() SIZE==0
[45488] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [14] comm_isend_on_stream() SIZE==0
ERR [10] comm_isend_on_stream() SIZE==0
[11458] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[38018] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[47704] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[37356] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[1475] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [15] comm_isend_on_stream() SIZE==0
[47413] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [8] comm_isend_on_stream() SIZE==0
[22569] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[40757] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
ERR [4] comm_isend_on_stream() SIZE==0
[5344] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
[5592] GDS INFO  gds_cq_map_smart() GDS_CQ_MAP_SMART env 0
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
Cuda failure gpu_kernels.cu:760: 'invalid configuration argument'
/home/hpcagos1/peersync/src/comd-cuda-async/bin/CoMD-cuda-mpi -e -i 16 -j 1 -k 1 -x 80 -y 80 -z 80  
--------------------------------------------------------------------------
mpirun has exited due to process rank 4 with PID 0 on
node tesla31 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.

--------------------------------------------------------------------------
+ date
mer 23 nov 2016, 09.38.34, GMT
